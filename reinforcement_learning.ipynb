{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Bcaxnx0JVaC"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "from collections import deque, namedtuple\n",
        "\n",
        "import gym\n",
        "import numpy as np\n",
        "import PIL.Image\n",
        "import tensorflow as tf\n",
        "import utils\n",
        "\n",
        "from pyvirtualdisplay import Display\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense, Input\n",
        "from tensorflow.keras.losses import MSE\n",
        "from tensorflow.keras.optimizers import Adam"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Display(visible=0, size=(840, 480)).start();\n",
        "\n",
        "tf.random.set_seed(utils.SEED)"
      ],
      "metadata": {
        "id": "5_kH-zEQJswc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MEMORY_SIZE = 100_000     # size of memory buffer\n",
        "GAMMA = 0.995             # discount factor\n",
        "ALPHA = 1e-3              # learning rate\n",
        "NUM_STEPS_FOR_UPDATE = 4  # perform a learning update every C time steps"
      ],
      "metadata": {
        "id": "PZgyRUuPJvM8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make('LunarLander-v2')"
      ],
      "metadata": {
        "id": "cYTDMUFHJwxP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env.reset()\n",
        "PIL.Image.fromarray(env.render(mode='rgb_array'))"
      ],
      "metadata": {
        "id": "ZHTQNg8cJ0YR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q_network = Sequential([\n",
        "    Input(shape=(state_size)),\n",
        "    Dense(units=64, activation='relu'),\n",
        "    Dense(units=64, activation='relu'),\n",
        "    Dense(units=num_actions, activation='linear')\n",
        "    ])\n",
        "\n",
        "target_q_network = Sequential([\n",
        "    Input(shape=(state_size)),\n",
        "    Dense(units=64, activation='relu'),\n",
        "    Dense(units=64, activation='relu'),\n",
        "    Dense(units=num_actions, activation='linear')\n",
        "    ])\n",
        "\n",
        "optimizer = Adam(learning_rate=ALPHA)"
      ],
      "metadata": {
        "id": "-FWc-dZ3J1uZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])"
      ],
      "metadata": {
        "id": "BfBOsTBvKEAx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_loss(experiences, gamma, q_network, target_q_network):\n",
        "    \"\"\"\n",
        "    Calculates the loss.\n",
        "\n",
        "    Args:\n",
        "      experiences: (tuple) tuple of [\"state\", \"action\", \"reward\", \"next_state\", \"done\"] namedtuples\n",
        "      gamma: (float) The discount factor.\n",
        "      q_network: (tf.keras.Sequential) Keras model for predicting the q_values\n",
        "      target_q_network: (tf.keras.Sequential) Keras model for predicting the targets\n",
        "\n",
        "    Returns:\n",
        "      loss: (TensorFlow Tensor(shape=(0,), dtype=int32)) the Mean-Squared Error between\n",
        "            the y targets and the Q(s,a) values.\n",
        "    \"\"\"\n",
        "\n",
        "    states, actions, rewards, next_states, done_vals = experiences\n",
        "    max_qsa = tf.reduce_max(target_q_network(next_states), axis=-1)\n",
        "\n",
        "    y_targets = (done_vals) * (rewards) + (1 - done_vals) * (rewards + gamma * max_qsa)\n",
        "    q_values = q_network(states)\n",
        "    q_values = tf.gather_nd(q_values, tf.stack([tf.range(q_values.shape[0]),\n",
        "                                                tf.cast(actions, tf.int32)], axis=1))\n",
        "\n",
        "    loss = MSE(y_targets, q_values)\n",
        "\n",
        "    return loss"
      ],
      "metadata": {
        "id": "NZWba_siKFCU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def agent_learn(experiences, gamma):\n",
        "    \"\"\"\n",
        "    Updates the weights of the Q networks.\n",
        "\n",
        "    Args:\n",
        "      experiences: (tuple) tuple of [\"state\", \"action\", \"reward\", \"next_state\", \"done\"] namedtuples\n",
        "      gamma: (float) The discount factor.\n",
        "\n",
        "    \"\"\"\n",
        "    with tf.GradientTape() as tape:\n",
        "        loss = compute_loss(experiences, gamma, q_network, target_q_network)\n",
        "\n",
        "    gradients = tape.gradient(loss, q_network.trainable_variables)\n",
        "\n",
        "    optimizer.apply_gradients(zip(gradients, q_network.trainable_variables))\n",
        "\n",
        "    utils.update_target_network(q_network, target_q_network)"
      ],
      "metadata": {
        "id": "mkI-B2CMKVvo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "\n",
        "num_episodes = 2000\n",
        "max_num_timesteps = 1000\n",
        "\n",
        "total_point_history = []\n",
        "\n",
        "num_p_av = 100\n",
        "epsilon = 1.0\n",
        "\n",
        "memory_buffer = deque(maxlen=MEMORY_SIZE)\n",
        "\n",
        "target_q_network.set_weights(q_network.get_weights())\n",
        "\n",
        "for i in range(num_episodes):\n",
        "\n",
        "    state = env.reset()\n",
        "    total_points = 0\n",
        "\n",
        "    for t in range(max_num_timesteps):\n",
        "\n",
        "        state_qn = np.expand_dims(state, axis=0)\n",
        "        q_values = q_network(state_qn)\n",
        "        action = utils.get_action(q_values, epsilon)\n",
        "\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "        memory_buffer.append(experience(state, action, reward, next_state, done))\n",
        "\n",
        "        update = utils.check_update_conditions(t, NUM_STEPS_FOR_UPDATE, memory_buffer)\n",
        "\n",
        "        if update:\n",
        "            experiences = utils.get_experiences(memory_buffer)\n",
        "\n",
        "            agent_learn(experiences, GAMMA)\n",
        "\n",
        "        state = next_state.copy()\n",
        "        total_points += reward\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    total_point_history.append(total_points)\n",
        "    av_latest_points = np.mean(total_point_history[-num_p_av:])\n",
        "\n",
        "    epsilon = utils.get_new_eps(epsilon)\n",
        "\n",
        "    print(f\"\\rEpisode {i+1} | Total point average of the last {num_p_av} episodes: {av_latest_points:.2f}\", end=\"\")\n",
        "\n",
        "    if (i+1) % num_p_av == 0:\n",
        "        print(f\"\\rEpisode {i+1} | Total point average of the last {num_p_av} episodes: {av_latest_points:.2f}\")\n",
        "\n",
        "    if av_latest_points >= 200.0:\n",
        "        print(f\"\\n\\nEnvironment solved in {i+1} episodes!\")\n",
        "        q_network.save('lunar_lander_model.h5')\n",
        "        break\n",
        "\n",
        "tot_time = time.time() - start\n",
        "\n",
        "print(f\"\\nTotal Runtime: {tot_time:.2f} s ({(tot_time/60):.2f} min)\")"
      ],
      "metadata": {
        "id": "6fhuuV2MKa07"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Suppress warnings from imageio\n",
        "import logging\n",
        "logging.getLogger().setLevel(logging.ERROR)"
      ],
      "metadata": {
        "id": "HswnKfTAKooD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filename = \"./videos/lunar_lander.mp4\"\n",
        "\n",
        "utils.create_video(filename, env, q_network)\n",
        "utils.embed_mp4(filename)"
      ],
      "metadata": {
        "id": "53wgDT97KqTH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}